{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kianoosh/anaconda3/envs/autooed/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "from random import seed\n",
    "from random import randint\n",
    "\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import yaml\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# default is to maximize the objectives\n",
    "import time as time\n",
    "import copy\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "\n",
    "# example of a gaussian process surrogate function\n",
    "from math import sin\n",
    "from math import pi\n",
    "import numpy as np\n",
    "from numpy import arange\n",
    "from numpy import asarray\n",
    "from numpy.random import normal\n",
    "from numpy.random import uniform\n",
    "from numpy.random import random\n",
    "from numpy import cov\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from warnings import catch_warnings\n",
    "from warnings import simplefilter\n",
    "from autooed.utils.sampling import lhs\n",
    "import random\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score\n",
    "\n",
    "from scipy.stats import pearsonr as pearsonr\n",
    "from scipy import ndimage, misc\n",
    "import pickle\n",
    "import re\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Descriptors\n",
    "from chainer_chemistry.datasets.molnet import get_molnet_dataset\n",
    "from datetime import datetime\n",
    "from autooed.utils.sampling import lhs\n",
    "from autooed.problem import build_problem\n",
    "from autooed.mobo import build_algorithm\n",
    "from autooed.utils.seed import set_seed\n",
    "from autooed.utils.initialization import generate_random_initial_samples, load_provided_initial_samples\n",
    "from autooed.utils.plot import plot_performance_space, plot_performance_metric\n",
    "from autooed.utils.plot import plot_performance_space_diffcolor\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from arguments import get_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the functions\n",
    "\n",
    "def get_general_args(args=None):\n",
    "    '''\n",
    "    General arguments: problem and algorithm description, experiment settings\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--problem', type=str, default='printing3d_dlp',\n",
    "                        help='optimization problem')\n",
    "    parser.add_argument('--algo', type=str, default='tsemo',\n",
    "                        help='type of algorithm to use with some predefined arguments, or custom arguments')\n",
    "    parser.add_argument('--seed', type=int, default=10,\n",
    "                        help='the specific seed to run')\n",
    "    parser.add_argument('--batch-size', type=int, default=2,\n",
    "                        help='size of the batch in optimization')\n",
    "    parser.add_argument('--n-init-sample', type=int, default=0,\n",
    "                        help='number of initial design samples')\n",
    "    parser.add_argument('--n-total-sample', type=int, default=50,\n",
    "                        help='number of total design samples (budget)')\n",
    "\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_surroagte_args(args=None):\n",
    "    '''\n",
    "    Arguments for fitting the surrogate model\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--surrogate', type=str,\n",
    "                        choices=['gp', 'nn', 'bnn'], default='gp',\n",
    "                        help='type of the surrogate model')\n",
    "\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_acquisition_args(args=None):\n",
    "    '''\n",
    "    Arguments for acquisition function\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--acquisition', type=str,\n",
    "                        choices=['identity', 'pi', 'ei', 'ucb', 'ts'], default='ts',\n",
    "                        help='type of the acquisition function')\n",
    "\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_solver_args(args=None):\n",
    "    '''\n",
    "    Arguments for multi-objective solver\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    # general solver\n",
    "    parser.add_argument('--solver', type=str,\n",
    "                        choices=['nsga2', 'moead', 'parego', 'discovery', 'ga', 'cmaes'], default='nsga2',\n",
    "                        help='type of the multiobjective solver')\n",
    "    parser.add_argument('--n-process', type=int, default=-1,\n",
    "                        help='number of processes to be used for parallelization')\n",
    "\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_selection_args(args=None):\n",
    "    '''\n",
    "    Arguments for sample selection\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--selection', type=str,\n",
    "                        choices=['direct', 'hvi', 'random', 'uncertainty'], default='hvi',\n",
    "                        help='type of selection method for a new batch')\n",
    "\n",
    "    args, _ = parser.parse_known_args(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    '''\n",
    "    Get arguments from all components\n",
    "    You can specify args-path argument to directly load arguments from the specified yaml file\n",
    "    '''\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--args-path', type=str, default=None,\n",
    "                        help='used for directly loading arguments from the path of the argument file')\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    if args.args_path is None:\n",
    "        general_args = get_general_args()\n",
    "        surroagte_args = get_surroagte_args()\n",
    "        acquisition_args = get_acquisition_args()\n",
    "        solver_args = get_solver_args()\n",
    "        selection_args = get_selection_args()\n",
    "\n",
    "        module_cfg = {\n",
    "            'surrogate': vars(surroagte_args),\n",
    "            'acquisition': vars(acquisition_args),\n",
    "            'solver': vars(solver_args),\n",
    "            'selection': vars(selection_args),\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        with open(args.args_path, 'r') as f:\n",
    "            all_args = yaml.load(f)\n",
    "\n",
    "        general_args = Namespace(**all_args['general'])\n",
    "        module_cfg = all_args.copy()\n",
    "        module_cfg.pop('general')\n",
    "\n",
    "    return general_args, module_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main optimization loop \n",
    " \n",
    "# Preprocessing: Reading and processing the data\n",
    "df = pd.read_csv('Yuchao_20220816.csv')\n",
    "\n",
    "# Process printability as Y and Tg\n",
    "Printability = np.asarray(df['Printability']).reshape(1, -1).T\n",
    "Y0 = Printability\n",
    "Y = np.where(Y0 == 'Y', 1, 0)\n",
    "\n",
    "Tg = np.asarray(df['Tg']).reshape(1, -1).T\n",
    "\n",
    "# Set a very high value for Tg that are not printable (too brittle)\n",
    "Tg[np.isnan(Tg)] = 200\n",
    "\n",
    "# Group Tg values, aiming for a range of [10, 60]\n",
    "Tg_group = [1 if 10 < i < 60 else 0 for i in Tg]\n",
    "Tg_group = np.array(Tg_group)\n",
    "\n",
    "# Read the objectives: toughness, strength, and strain\n",
    "toughness = np.asarray(df['Toughness(MJ/m3)']).reshape(1, -1).T\n",
    "toughness[np.isnan(toughness)] = 0\n",
    "\n",
    "strength = np.asarray(df['Tensile_Strength(MPa)']).reshape(1, -1).T\n",
    "strength[np.isnan(strength)] = 0\n",
    "\n",
    "# Not using Tensile strain as an objective\n",
    "strain = np.asarray(df['Tensile_Strain_percentage']).reshape(1, -1).T\n",
    "strain[np.isnan(strain)] = 0\n",
    "\n",
    "# Read the ratios of 6 monomers.\n",
    "A_Ratio = np.asarray(df['R1(HA)']).reshape(1, -1)\n",
    "B_Ratio = np.asarray(df['R2(IA)']).reshape(1, -1)\n",
    "C_Ratio = np.asarray(df['R3(NVP)']).reshape(1, -1)\n",
    "D_Ratio = np.asarray(df['R4(AA)']).reshape(1, -1)\n",
    "E_Ratio = np.asarray(df['R5(HEAA)']).reshape(1, -1)\n",
    "F_Ratio = np.asarray(df['R6(IBOA)']).reshape(1, -1)\n",
    "\n",
    "# Combine the monomer ratios\n",
    "X0 = np.concatenate((A_Ratio.T, B_Ratio.T, C_Ratio.T,\n",
    "                    D_Ratio.T, E_Ratio.T, F_Ratio.T), axis=1)\n",
    "\n",
    "# Load monomer descriptors from 'monomers_info.csv'\n",
    "df = pd.read_csv('./Datasets/monomers_info.csv')\n",
    "energy = np.array(-df['dft_sp_E_RB3LYP'])\n",
    "pol_area = np.array(df['polar_surface_area'])\n",
    "complexity = np.array(df['complexity'])\n",
    "HAMW = np.array(df['HAMW'])\n",
    "solubility = np.array(df['solubility_sqrt_MJperm3'])\n",
    "solubility_d = np.array(df['solubility_dipole'])\n",
    "solubility_h = np.array(df['solubility_h'])\n",
    "solubility_p = np.array(df['solubility_p'])\n",
    "\n",
    "# Multiply monomer ratios by their descriptors\n",
    "X_energy = np.dot(X0, energy)\n",
    "X_complexity = np.dot(X0, complexity)\n",
    "X_HAMW = np.dot(X0, HAMW)\n",
    "X_solubility_d = np.dot(X0, solubility_d)\n",
    "X_solubility_h = np.dot(X0, solubility_h)\n",
    "X_solubility_p = np.dot(X0, solubility_p)\n",
    "\n",
    "# Combine all features\n",
    "X = np.concatenate((X_energy.reshape(-1, 1), X_complexity.reshape(-1, 1), X_HAMW.reshape(-1, 1),\n",
    "                    X_solubility_d.reshape(-1, 1), X_solubility_h.reshape(-1, 1), X_solubility_p.reshape(-1, 1)), axis=1)\n",
    "\n",
    "\n",
    "# Train a random forest classifier for printability\n",
    "RF_print = RandomForestClassifier(random_state=0, criterion=\"gini\",\n",
    "                                  max_depth=5,\n",
    "                                  n_estimators=50)\n",
    "RF_print.fit(X, Y.ravel())\n",
    "Yhat = RF_print.predict(X)\n",
    "acc = accuracy_score(Y, Yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "\n",
    "# Train a random forest classifier for Tg grouping\n",
    "RF_Tg = RandomForestClassifier(random_state=0, criterion=\"gini\",\n",
    "                               max_depth=5,\n",
    "                               n_estimators=50)\n",
    "RF_Tg.fit(X, Tg_group)\n",
    "Yhat = RF_Tg.predict(X)\n",
    "acc = accuracy_score(Tg_group, Yhat)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "\n",
    "# Load and set up optimization arguments\n",
    "args, module_cfg = get_args()\n",
    "print(args.seed)\n",
    "\n",
    "# Set the random seed\n",
    "set_seed(args.seed)\n",
    "\n",
    "# Build the optimization problem\n",
    "problem = build_problem(args.problem)\n",
    "print(problem)\n",
    "\n",
    "# Build the optimization algorithm\n",
    "algorithm = build_algorithm(args.algo, problem, module_cfg)\n",
    "print(algorithm)\n",
    "\n",
    "# Generate initial random samples or load provided samples\n",
    "X = generate_random_initial_samples(problem, args.n_init_sample)\n",
    "Y = np.array([problem.evaluate_objective(x) for x in X])\n",
    "print('read X', X.shape)\n",
    "print('read Y', Y.shape)\n",
    "\n",
    "# Load the initial samples (Ratios) and two objectives: Strength, Toughness\n",
    "path = ['./Datasets/Yuchao_20220816_X.csv',\n",
    "        './Datasets/Yuchao_20220816_Y.csv']\n",
    "X, Y = load_provided_initial_samples(path)\n",
    "\n",
    "# Since we minimize the objectives, multiply by -1\n",
    "Y = -Y\n",
    "print('read X', X.shape)\n",
    "print('read Y', Y.shape)\n",
    "\n",
    "X0 = X\n",
    "Y0 = Y\n",
    "\n",
    "# Optimization loop\n",
    "while len(X) < args.n_total_sample:\n",
    "    start = time.time()\n",
    "\n",
    "    # Propose design samples\n",
    "    X_next = algorithm.optimize(X, Y, X_busy=None, batch_size=2)\n",
    "    print(X_next)\n",
    "    print(time.time() - start)\n",
    "\n",
    "    # Evaluate proposed samples\n",
    "    Y_next = np.array([problem.evaluate_objective(x) for x in X_next])\n",
    "\n",
    "    # Combine into the dataset\n",
    "    X = np.vstack([X, X_next])\n",
    "    Y = np.vstack([Y, Y_next])\n",
    "\n",
    "    for (x_next, y_next) in zip(X_next, Y_next):\n",
    "        while True:\n",
    "            try:\n",
    "                printability_new = int(input(\n",
    "                    \"ratios A-F {} sum {} Enter Printability 0or1: \".\n",
    "                    format(np.round(x_next, 2), np.sum(np.round(x_next, 2)))))\n",
    "            except ValueError:\n",
    "                print(\"printability is not read correctly\")\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                Tg_new = float(input(\n",
    "                    \"ratios A-F {} sum {} Enter Tg: \".\n",
    "                    format(np.round(x_next, 2), np.sum(np.round(x_next, 2)))))\n",
    "            except ValueError:\n",
    "                print(\"Tg is not read correctly\")\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        new_printability_Tg = [list(np.round(x_next, 2)), [\n",
    "            1-np.sum(np.round(x_next, 2))], printability_new, Tg_new]\n",
    "        \n",
    "        new_sample = new_printability_Tg\n",
    "        new_printability_Tg = str(new_printability_Tg)\n",
    "        new_printability_Tg = new_printability_Tg.replace(\"[\", \"\")\n",
    "        new_printability_Tg = new_printability_Tg.replace(\"]\", \"\")\n",
    "        new_printability_Tg = new_printability_Tg + \"\\n\"\n",
    "\n",
    "        with open('./Datasets/printability_Tg.csv', 'a') as fd:\n",
    "            fd.write(new_printability_Tg)\n",
    "\n",
    "        new_sample.append(y_next[0])\n",
    "        new_sample.append(y_next[1])\n",
    "        new_sample = str(new_sample)\n",
    "        new_sample = new_sample.replace(\"[\", \"\")\n",
    "        new_sample = new_sample.replace(\"]\", \"\")\n",
    "        new_sample = new_sample + \"\\n\"\n",
    "\n",
    "        with open('./Datasets/new_evaluated.csv', 'a') as fd:\n",
    "            fd.write(new_sample)\n",
    "\n",
    "    print(f'{len(X)}/{args.n_total_sample} complete')\n",
    "    print(time.time() - start)\n",
    "\n",
    "\n",
    "# Plot the performance metrics\n",
    "Y_eval = Y[Y0.shape[0]:, :]\n",
    "plot_performance_space_diffcolor(Y0=-Y0, Y_eval=-Y_eval)\n",
    "plot_performance_metric(Y, problem.obj_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autooed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
